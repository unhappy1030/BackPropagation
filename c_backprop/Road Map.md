
### 📚 학습 및 구현 로드맵

1. **이론 탄탄히 다지기 (개념)**
    
    - **순전파 (Forward Propagation):** 입력값(x)이 가중치(w)와 편향(b)을 만나 활성화 함수(Activation Function)를 거쳐 출력값(ŷ)을 만들어내는 과정을 완벽히 이해해야 해요.
        
    - **손실 함수 (Cost/Loss Function):** 예측값(ŷ)과 실제값(y)의 차이, 즉 '오차'를 어떻게 숫자로 측정하는지 알아야 해요. (평균 제곱 오차 - MSE가 대표적이죠.)
        
    - **역전파 (Backpropagation)의 핵심 원리:** 이 오차를 줄이기 위해 각 가중치(w)와 편향(b)이 오차에 얼마나 영향을 미쳤는지(기여도)를 계산하는 과정이에요. 여기서 바로 미분과 **연쇄 법칙(Chain Rule)**이 핵심적인 역할을 합니다. 🧠
        
2. **C언어로 구현 준비 (기반 다지기)**
    
    - **자료구조 설계:** 신경망, 레이어, 뉴런을 C언어에서 어떻게 표현할지 고민해야 해요. 보통 `struct`를 사용해서 깔끔하게 구조를 짤 수 있어요.
        
    - **행렬 연산 함수:** 신경망 연산의 대부분은 행렬 연산이에요. 행렬의 곱셈, 덧셈, 전치(transpose) 등을 수행하는 함수들을 미리 만들어둬야 합니다.
        
    - **수학 함수:** 활성화 함수(예: Sigmoid, ReLU)와 그 함수의 미분값을 계산하는 함수들을 준비해야 해요.
        
3. **코드 구현 (한 단계씩!)**
    
    - **1단계: 순전파 구현:** 입력층, 은닉층, 출력층을 거치며 최종 예측값을 계산하는 코드를 작성해요.
        
    - **2단계: 오차 계산:** 출력층에서 손실 함수를 이용해 오차를 계산해요.
        
    - **3단계: 역전파 구현 (핵심!):**
        
        - 출력층의 가중치와 편향에 대한 그래디언트(gradient, 오차의 기울기)를 계산해요.
            
        - 연쇄 법칙을 이용해 은닉층의 그래디언트를 계산해요. (출력층에서 계산한 그래디언트를 뒤로 전달!)
            
    - **4단계: 가중치 및 편향 업데이트:** 계산된 그래디언트와 학습률(learning rate)을 이용해 신경망의 모든 가중치와 편향을 업데이트해요.
        
    - **5단계: 전체 학습 루프:** 이 모든 과정을 반복(epoch)하면서 오차가 점점 줄어드는지 확인하고, 간단한 데이터셋(예: XOR 문제)으로 테스트해요.
        
